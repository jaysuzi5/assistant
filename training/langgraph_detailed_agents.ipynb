{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce59910",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e518321",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LangGraph Detailed Agent\n",
    "This advances the agent that was done in the langgraph_agent.ipynb notebook and focuses on defining specific roles to the agents/nodes.  Review that one first to see details on the steps as this one will only explain the additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739f0d3",
   "metadata": {},
   "source": [
    "### 1. Load the needed libraries and environment variables\n",
    "Unlike most notebooks, I am not going to load all of the imports at the top, but load them where they are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import List, Any, Optional, Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import uuid\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import gradio \n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_community.tools import Tool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41e526",
   "metadata": {},
   "source": [
    "### 2. Add Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbceb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tools() -> List[BaseTool]:\n",
    "    # Define server, wrapper around the Google API, as a tool\n",
    "    tool_search: Tool = Tool(\n",
    "        name=\"search\",\n",
    "        func=GoogleSerperAPIWrapper().run,\n",
    "        description=\"Use this tool when you want to get the results of an online web search\"\n",
    "    )\n",
    "\n",
    "    # Define Wikipedia as a search as a tool\n",
    "    wikipedia: WikipediaAPIWrapper = WikipediaAPIWrapper()\n",
    "    wiki_tool: WikipediaQueryRun = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "\n",
    "    # Define file management tools\n",
    "    file_tools: List[BaseTool] = FileManagementToolkit(root_dir=\"sandbox\").get_tools()\n",
    "\n",
    "    # Concatenate all tools together as a list\n",
    "    tools: List[BaseTool] = file_tools + [tool_search, wiki_tool]\n",
    "\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043a674",
   "metadata": {},
   "source": [
    "### 3. Setup Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"memory.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "sql_memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79179fae",
   "metadata": {},
   "source": [
    "### 4. Build the Graph\n",
    "It is now time to put start building the graph.  All graphs include the following steps:\n",
    "<ol>\n",
    "<li>Define the State</li>\n",
    "<li>Start Graph Builder</li>\n",
    "<li>Create Nodes</li>\n",
    "<li>Create Edges</li>\n",
    "<li>Compile the Graph</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a48ed",
   "metadata": {},
   "source": [
    "### State with additional details\n",
    "Will track additional details in the state to help with routing:\n",
    "<ul>\n",
    "<li>feedback_on_work: details from the evaluator</li>\n",
    "<li>success_criteria_met: determines if it should end</li>\n",
    "<li>reviews: the number of reviews completed as a guardrail from looping too much</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the State object\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    reviews: int=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Start the Graph Builder with this State class\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d351176",
   "metadata": {},
   "source": [
    "### Detailed Agent Setup\n",
    "In this case, we are going to define two agents/nodes with different roles.  The first one will be a \"history\" expert.  The purpose of this agent is that it can do research on historical events and create a detailed report that will be saved as a markdown file.  This will be given a consistent prompt and the user should only have to supply an event.  The second agent/node is an evaluator.  This evaluator will review the report and ensure provide feedback.  Only after it is statified with the results will the process end.  This will also use two different LLMs with the history export using OpenAI and the reviewer using Gemini.  Each of these agents could also be set up with different tool sets as well, however, in this case, I will make the tools available to both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185838",
   "metadata": {},
   "source": [
    "#### Define the prompts\n",
    "I will first define a detailed prompt for both agents.  These may very well need to be fined tuned to get them right.  You can use AI to get a jump start on crafting a good system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_expert_system_message = \"\"\"\n",
    "You are HistoryExpert, an autonomous research agent specializing in historical events. You have access to tools including Google Search, Wikipedia, and file management (read_file, write_file, list_directory, delete_file). Your mission is to take a single historical event as input and generate a deep, comprehensive, meticulously accurate historical report about that event, and SAVE IT TO DISK.\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "\n",
    "1. Begin with a concise executive summary. This summary must contain 3–6 bullet points capturing the essential who, what, when, where, why, and outcome of the event. It must be clearly labeled with the header: \"## Executive Summary\".\n",
    "\n",
    "2. After the summary, produce a detailed, lengthy, deeply researched historical report. Use section headers where appropriate. The report must include background, causes, chronology, major participants, significance, geopolitical impact, cultural or social consequences, and primary sources when available. Cite sources (Wikipedia or Google) in plain text, not with citation IDs or URLs unless required.\n",
    "\n",
    "3. IMPORTANT - WRITE THE REPORT TO DISK: After creating the report content, you MUST use the write_file tool to save it. The filename should follow this exact format:\n",
    "   FILENAME: YYYYMMDDHHmm_<event_name>.md\n",
    "   Where YYYYMMDDHHmm is the current UTC timestamp (use today's date), and <event_name> is the event name in snake_case with no spaces.\n",
    "   \n",
    "   Example: If the event is \"Battle of Hastings\" today is 2024-11-15 at 14:30 UTC, the file should be saved as:\n",
    "   \"20241115143000_battle_of_hastings.md\"\n",
    "\n",
    "4. Tone requirements: Maintain an authoritative, neutral, academic, objective tone. Note when historians disagree. Avoid speculation unless it is explicitly labeled as speculation.\n",
    "\n",
    "BEHAVIOR RULES:\n",
    "- Prefer primary historical sources first, then Wikipedia, then Google.\n",
    "- If the event name is ambiguous, identify any possible interpretations and ask for clarification if absolutely necessary.\n",
    "- The report must always be highly detailed unless explicitly instructed otherwise.\n",
    "- Do not produce a summary at the bottom; only at the top.\n",
    "- ALWAYS save the file using the write_file tool when done - this is mandatory.\n",
    "\n",
    "FINAL WORKFLOW:\n",
    "1. Research the event using search and wikipedia tools if needed\n",
    "2. Create the full report content (Executive Summary + detailed report)\n",
    "3. Use write_file tool to save the report with the properly formatted filename\n",
    "4. Confirm the file has been saved\n",
    "\"\"\"\n",
    "\n",
    "evaluator_system_message = \"\"\"\n",
    "You are HistoryReviewAgent, an autonomous quality-control reviewer responsible for evaluating the output generated by the HistoryExpert agent. You must evaluate the historical report strictly using the structured fields defined in the EvaluatorOutput model:\n",
    "- feedback (string)\n",
    "- success_criteria_met (boolean)\n",
    "- reviews (integer)\n",
    "\n",
    "Your responsibilities:\n",
    "\n",
    "1. Evaluate whether the report fully complies with the required structure:\n",
    "   - Contains an \"## Executive Summary\" at the top with 3–6 bullet points.\n",
    "   - Contains a lengthy, detailed historical report covering:\n",
    "       background, causes, chronology, major participants, significance,\n",
    "       geopolitical impact, cultural/social consequences, and sources.\n",
    "   - Uses section headers appropriately.\n",
    "   - Maintains an objective, academic, historically accurate tone.\n",
    "   - Avoids speculation unless explicitly labeled as such.\n",
    "   - Has been saved to disk as a file (use read_file tool to verify the file exists in the sandbox directory).\n",
    "\n",
    "2. If ALL success criteria are met:\n",
    "   - Set success_criteria_met = true\n",
    "   - Set feedback to a short confirmation message such as:\n",
    "       \"Report meets all criteria and has been saved to disk.\"\n",
    "   - Set reviews to any integer value (your code will override/increment it).\n",
    "\n",
    "3. If ANY criteria are NOT met:\n",
    "   - Set success_criteria_met = false\n",
    "   - Set feedback to clear, constructive, actionable correction notes that the HistoryExpert can use.\n",
    "   - Do NOT reference review numbers or counts.\n",
    "   - Set reviews to any integer value (your code will override/increment it).\n",
    "\n",
    "IMPORTANT:\n",
    "- The agent should NOT attempt to track, reference, or reason about review counts.\n",
    "- The evaluator MUST NOT rewrite or fix the report — only evaluate it.\n",
    "- The evaluator MUST NOT include a file name in its response.\n",
    "- Use list_directory and read_file tools to verify the file was actually written to disk.\n",
    "- The only output should be the structured fields required by EvaluatorOutput.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the history expert\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "open_ai = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools_for_history_expert = get_tools()\n",
    "open_ai_with_tools = open_ai.bind_tools(tools_for_history_expert)\n",
    "\n",
    "def history_expert(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Convert incoming messages to BaseMessage objects\n",
    "    converted_messages = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, dict):\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                converted_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                converted_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                converted_messages.append(AIMessage(content=msg.get(\"content\", \"\"), tool_calls=msg.get(\"tool_calls\")))\n",
    "            elif msg[\"role\"] == \"tool\":\n",
    "                converted_messages.append(ToolMessage(content=msg[\"content\"], tool_call_id=msg.get(\"tool_call_id\")))\n",
    "        else:\n",
    "            converted_messages.append(msg)\n",
    "\n",
    "    # Ensure system message is present\n",
    "    has_system_message = any(isinstance(m, SystemMessage) for m in converted_messages)\n",
    "    if not has_system_message:\n",
    "        converted_messages.insert(0, SystemMessage(content=history_expert_system_message))\n",
    "\n",
    "    # Invoke the LLM with the properly formatted messages\n",
    "    response = open_ai_with_tools.invoke(converted_messages)\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Add history expert\n",
    "graph_builder.add_node(\"history_expert\", history_expert)\n",
    "\n",
    "# Create a custom tool node that properly handles tool execution\n",
    "def tools_for_history_expert_node(state: State):\n",
    "    \"\"\"Execute tools and add tool responses to messages\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message doesn't have tool_calls, something is wrong\n",
    "    if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    # Execute each tool call\n",
    "    tool_responses = []\n",
    "    tool_map = {tool.name: tool for tool in tools_for_history_expert}\n",
    "    \n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call.get(\"name\") or tool_call.get(\"type\")\n",
    "        # Handle both dict-style and object-style tool calls\n",
    "        if isinstance(tool_call, dict):\n",
    "            tool_input = tool_call.get(\"args\", {})\n",
    "        else:\n",
    "            tool_input = tool_call.args if hasattr(tool_call, 'args') else {}\n",
    "        \n",
    "        try:\n",
    "            if tool_name in tool_map:\n",
    "                result = tool_map[tool_name].invoke(tool_input)\n",
    "            else:\n",
    "                result = f\"Tool {tool_name} not found\"\n",
    "        except Exception as e:\n",
    "            result = f\"Error executing {tool_name}: {str(e)}\"\n",
    "        \n",
    "        tool_responses.append(\n",
    "            ToolMessage(\n",
    "                content=str(result),\n",
    "                tool_call_id=tool_call.get(\"id\") if isinstance(tool_call, dict) else tool_call.id\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": tool_responses}\n",
    "\n",
    "graph_builder.add_node(\"tools_for_history_expert\", tools_for_history_expert_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca81592",
   "metadata": {},
   "source": [
    "#### Structured Output\n",
    "For the evaluator, we will define the structure that should be returned.  This will make routing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluator with structured output\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    reviews: int = Field(description=\"The number of reviews that have been conducted.  Increase this by one from the previous amount.\")   \n",
    "\n",
    "tools_for_evaluator = get_tools()\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "gemini_with_tools = gemini.bind_tools(tools_for_evaluator)\n",
    "gemini_structured_with_tools = gemini_with_tools.with_structured_output(EvaluatorOutput)\n",
    "\n",
    "\n",
    "def evaluator(state: State):\n",
    "    # Add in the system message\n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = evaluator_system_message\n",
    "            found_system_message = True\n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=evaluator_system_message)] + messages\n",
    "\n",
    "    eval_result = gemini_structured_with_tools.invoke(messages)\n",
    "    new_state = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Evaluator Feedback on this answer: {eval_result.feedback}\",\n",
    "            }\n",
    "        ],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"reviews\": eval_result.reviews + 1,\n",
    "    }\n",
    "    return new_state\n",
    "\n",
    "\n",
    "# Add evaluator\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "# Create a custom tool node for evaluator\n",
    "def tools_for_evaluator_node(state: State):\n",
    "    \"\"\"Execute tools for evaluator and add tool responses to messages\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message doesn't have tool_calls, something is wrong\n",
    "    if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    # Execute each tool call\n",
    "    tool_responses = []\n",
    "    tool_map = {tool.name: tool for tool in tools_for_evaluator}\n",
    "    \n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call.get(\"name\") or tool_call.get(\"type\")\n",
    "        # Handle both dict-style and object-style tool calls\n",
    "        if isinstance(tool_call, dict):\n",
    "            tool_input = tool_call.get(\"args\", {})\n",
    "        else:\n",
    "            tool_input = tool_call.args if hasattr(tool_call, 'args') else {}\n",
    "        \n",
    "        try:\n",
    "            if tool_name in tool_map:\n",
    "                result = tool_map[tool_name].invoke(tool_input)\n",
    "            else:\n",
    "                result = f\"Tool {tool_name} not found\"\n",
    "        except Exception as e:\n",
    "            result = f\"Error executing {tool_name}: {str(e)}\"\n",
    "        \n",
    "        tool_responses.append(\n",
    "            ToolMessage(\n",
    "                content=str(result),\n",
    "                tool_call_id=tool_call.get(\"id\") if isinstance(tool_call, dict) else tool_call.id\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": tool_responses}\n",
    "\n",
    "graph_builder.add_node(\"tools_for_evaluator\", tools_for_evaluator_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf0bb",
   "metadata": {},
   "source": [
    "### Define condition statements.  This is even necessary for the tools_condition since we have different versions of the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER_OF_REVIEWS = 4\n",
    "\n",
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"reviews\"] >= MAX_NUMBER_OF_REVIEWS:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"history_expert\"\n",
    "\n",
    "\n",
    "def tools_condition_history_expert(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools_for_history_expert\"\n",
    "    return \"evaluator\"\n",
    "\n",
    "\n",
    "def tools_condition_evaluator(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools_for_evaluator\"\n",
    "    return \"done\"  # Changed from \"evaluator\" to \"done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02042800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the edges\n",
    "\n",
    "# From history_expert: if needs tools, go to tools; otherwise go to evaluator\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"history_expert\", \n",
    "    tools_condition_history_expert, \n",
    "    {\"tools_for_history_expert\": \"tools_for_history_expert\", \"evaluator\": \"evaluator\"}\n",
    ")\n",
    "\n",
    "# From tools_for_history_expert: always back to history_expert to process tool results\n",
    "graph_builder.add_edge(\"tools_for_history_expert\", \"history_expert\")\n",
    "\n",
    "# From evaluator: if needs tools, go to tools; otherwise check if done\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"evaluator\", \n",
    "    tools_condition_evaluator, \n",
    "    {\"tools_for_evaluator\": \"tools_for_evaluator\", \"done\": \"done\"}\n",
    ")\n",
    "\n",
    "# From tools_for_evaluator: always back to evaluator to process tool results  \n",
    "graph_builder.add_edge(\"tools_for_evaluator\", \"evaluator\")\n",
    "\n",
    "# From done node: route to history_expert or END\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"done\",\n",
    "    route_based_on_evaluation,\n",
    "    {\"history_expert\": \"history_expert\", \"END\": END}\n",
    ")\n",
    "\n",
    "# Entry point\n",
    "graph_builder.add_edge(START, \"history_expert\")\n",
    "\n",
    "# Add done node to handle routing decision\n",
    "def done_node(state: State):\n",
    "    \"\"\"Placeholder node for routing logic\"\"\"\n",
    "    return {}\n",
    "\n",
    "graph_builder.add_node(\"done\", done_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the graph\n",
    "graph = graph_builder.compile(checkpointer=sql_memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032120e",
   "metadata": {},
   "source": [
    "### 5. Test it Out\n",
    "Use Gradio to create a front end that will allow us to test this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_key = \"jay_123\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "def chat(user_input: str, history):\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gradio.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
